<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>赵江伟的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="description" content="KG-BERT: BERT for Knowledge Graph Completion(2019)
这篇文章是介绍知识库补全方面的工作, 结合预训练模型BERT可以将更丰富的上下文表示结合进模型中, 在三元组分类、链接预测以及关系预测等任务中达到了SOTA效果.
具体的做法也非常简单易懂, 就是修">
  
  
  
    <link rel="shortcut icon" href="../../../../favicon.ico">
  
  
    
<link rel="stylesheet" href="../../../../fancybox/jquery.fancybox-1.3.4.css">

  
  
<link rel="stylesheet" href="../../../../css/style.css">

  <meta name="google-site-verification" content="s7zZsgTqZwqYOoqReZl1ZE6FOOsSN0slhQFB9RTy0ag" />
  <meta name="baidu-site-verification" content="code-yqz8yGm4Fd" />
<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="../../../../index.html" id="logo">赵江伟的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="../../../../index.html">Home</a>
        
          <a class="main-nav-link" href="../../../../archives">Archives</a>
        
          <a class="main-nav-link" href="../../../../about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-bert+knowledge" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.619Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="**[kg-bert:-bert for knowledge graph completion(2019)](https://linkzhihu.com/?target=https%3a//arxiv.org/abs/1909.03193)**"><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.03193">KG-BERT: BERT for Knowledge Graph Completion(2019)</a></strong></h2>
<p>这篇文章是介绍知识库补全方面的工作, 结合预训练模型BERT可以将更丰富的上下文表示结合进模型中, 在三元组分类、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E9%93%BE%E6%8E%A5%E9%A2%84%E6%B5%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">链接预测</a>以及关系预测等任务中达到了SOTA效果.</p>
<p>具体的做法也非常简单易懂, 就是修改了BERT模型的输入使其适用于知识库三元组的形式.</p>
<p><img src="https://pic1.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_b.jpg" alt="img">首先是<strong>KG-BERT(a)</strong>, 输入为三元组 <span class="markdown-them-math-inline">$(h,r,t)$</span>的形式, 当然还有BERT自带的special tokens. 举个栗子, 对于三元组  <span class="markdown-them-math-inline">$(SteveJobs,foouned,AppleInc)$</span>, 上图中的<code>Head Entity</code>输入可以表示为<code>Steven Paul Jobs was an American business magnate, entrepreneur and investor</code>或者<code>Steve Jobs</code>, 而<code>Tail Entity</code>可以表示为<code>Apple Inc. is an American multinational technology company headquartered in Cupertino, California</code>或<code>Apple Inc</code>. 也就是说, 头尾实体的输入可以是<strong>实体描述</strong>句子或者<strong>实体名</strong>本身.</p>
<p>模型训练是首先分别构建**<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=positive+triple+set&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">positive triple set</a><strong>和</strong>negative triple set**, 然后用BERT的[CLS]标签做一个sigmoid打分以及最后交叉熵损失</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%2B%5Ccup+%5Cmathbb%7BD%7D%5E%7B-%7D%7D%5Cleft%28y_%7B%5Ctau%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+0%7D%5Cright%29%2B%5Cleft%281-y_%7B%5Ctau%7D%5Cright%29+%5Clog+%5Cleft%28s_%7B%5Ctau+1%7D%5Cright%29%5Cright%29" alt="[公式]"></p>
<p><img src="https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_b.jpg" alt="img"></p>
<p>上述的<strong>KG-BERT(a)<strong>需要输入关系, 对于关系分类任务不适用, 于是作者又提出一种</strong>KG-BERT(b)</strong>, 如上图. 这里只是把sigmoid的二分类改成了softmax的关系多分类.</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%5E%7B%5Cprime%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%5E%7B%2B%7D%7D+%5Csum_%7Bi%3D1%7D%5E%7BR%7D+y_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D%5Cright%29" alt="[公式]"></p>
<h2 id="**[k-bert:-enabling language representation with knowledge graph(2019)](https://linkzhihu.com/?target=https%3a//arxiv.org/abs/1909.07606)**"><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.07606">K-BERT: Enabling Language Representation with Knowledge Graph(2019)</a></strong></h2>
<p>作者指出通过公开语料训练的BERT模型仅仅是获得了general knowledge, 就像是一个普通人, 当面对特定领域的情境时 (如医疗、金融等) , 往往表现不如意, 即<strong>domain discrepancy</strong>. 而本文提出的<strong>K-BERT</strong>则像是领域专家, 通过将知识库中的结构化信息 (三元组) 融入到预训练模型中, 可以更好地解决领域相关任务. 如何将外部知识整合到模型中成了一个关键点, 这一步通常存在两个难点:</p>
<ul>
<li>**Heterogeneous Embedding Space: ** 即文本的单词embedding和知识库的实体实体embedding通常是通过不同方式获取的, 使得他俩的向量空间不一致;</li>
<li>**Knowledge Noise: ** 即过多的知识融合可能会使原始句子偏离正确的本意, 过犹不及.</li>
</ul>
<p>模型的整体框架如下图, 主要包括了四个子模块:  <strong>knowledge layer</strong>, <strong>embedding layer</strong>, <strong>seeing layer</strong> 和 <strong>mask-transformer</strong>. 对于一个给定的输入 <span class="markdown-them-math-inline">$s={w_0,w_1,w_2,...,w_n}$</span>, 首先 <strong>knowledge layer</strong>会从一个KG中注入相关的三元组, 将原来的句子转换成一个knowledge-rich的句子树; 接着句子树被同时送入<strong>embedding layer</strong>和<strong>seeing layer</strong>生成一个token级别的embedding表示和一个可见矩阵 (visible matrix); 最后通过<strong>mask-transformer</strong>层编码后用于下游任务的输出.</p>
<p><img src="https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_b.jpg" alt="img"></p>
<h3 id="**knowledge-layer**"><strong>Knowledge Layer</strong></h3>
<p>这一层的输入是原始句子 <span class="markdown-them-math-inline">$s=\{w_0,w_1,w_2,...,w_0\}$</span> , 输出是融入KG信息后的句子树<span class="markdown-them-math-inline">$t=\{w_0,w_1,...,w_i\{(r_{i,0},w_{i,0},...,r_{i,k},w_{i,k})\},...w_n\}$</span></p>
<p>通过两步完成:</p>
<ul>
<li><strong>K-Query</strong> 输入句子中涉及的所有实体都被选中, 并查询它们在KG中对应的三元组 <span class="markdown-them-math-inline">$E$</span> ;</li>
<li><strong>K-Inject</strong> 将查询到的三元组注入到句子<span class="markdown-them-math-inline">$S$</span> 中, 将  <span class="markdown-them-math-inline">$E$</span>中的三元组插入到它们相应的位置, 并生成一个句子树  <span class="markdown-them-math-inline">$t$</span>.</li>
</ul>
<h3 id="**embedding-layer**"><strong>Embedding Layer</strong></h3>
<p>K-BERT的输入和原始BERT的输入形式是一样的, 都需要token embedding, position embedding和segment embedding, 不同的是, K-BERT的输入是一个句子树, 因此问题就变成了句子树到序列化句子的转化, 并同时保留结构化信息.</p>
<p><img src="https://pic4.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_b.jpg" alt="img"><strong>Token embedding</strong></p>
<p>句子树的序列化, 作者提出一种简单的重排策略: <strong>分支中的token被插入到相应节点之后, 而后续的token被向后移动</strong>. 举个栗子, 对于上图中的句子树, 则重排后变成了<code>Tim Cook CEO Apple is visiting Beijing capital China is a City now</code>. 没错, 这的确看上去毫无逻辑, 但是还好后面可以通过trick来解决.</p>
<h3 id="**soft-position-embedding**"><strong>Soft-position embedding</strong></h3>
<p>通过重排后的句子显然是毫无意义的, 这里利用了position embedding来还原回结构信息. 还是以上图为例, 重排后, <code>CEO</code>和<code>Apple</code>被插入在了<code>Cook</code>和<code>is</code>之间, 但是<code>is</code>应该是接在<code>Cook</code>之后一个位置的, 那么我们直接把<code>is</code>的position number 设置为3即可. Segment embedding 部分同BERT一样.</p>
<h3 id="**seeing-layer**"><strong>Seeing Layer</strong></h3>
<p>作者认为Seeing layer的mask matrix是K-BERT有效的关键, 主要解决了前面提到的<strong>Knowledge Noise</strong>问题. 栗子中<code>China</code>仅仅修饰的是<code>Beijing</code>, 和<code>Apple</code>半毛钱关系没有, 因此像这种token之间就不应该有相互影响. 为此定义一个可见矩阵, 判断句子中的单词之间是否彼此影响</p>
<p><img src="https://www.zhihu.com/equation?tex=M_%7Bi+j%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7B0%7D%26%7Bw_%7Bi%7D+%5Cominus+w_%7Bj%7D%7D+%5C%5C+%7B-%5Cinfty%7D+%26+%7Bw_%7Bi%7D+%5Coslash+w_%7Bj%7D%7D%5Cend%7Barray%7D%5Cright." alt="[公式]"></p>
<h3 id="**mask-transformer**"><strong>Mask-Transformer</strong></h3>
<p>BERT中的Transformer Encoder不能接受上述可见矩阵作为输入, 因此需要稍作改进. Mask-Transformer是一层层<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=mask-self-attention&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">mask-self-attention</a>的堆叠,</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D+%7B+c+%7D+%7B+Q+%5E+%7B+i+%2B+1+%7D+%2C+K+%5E+%7B+i+%2B+1+%7D+%2C+V+%5E+%7B+i+%2B+1+%7D+%3D+h+%5E+%7B+i+%7D+W+_+%7B+q+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+k+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+v+%7D+%7D+%5C%5C+%7B+S+%5E+%7B+i+%2B+1+%7D+%3D+%5Coperatorname+%7B+softmax+%7D+%5Cleft%28+%5Cfrac+%7B+Q+%5E+%7B+i+%2B+1+%7D+K+%5E+%7B+i+%2B+1+%7D+%2B+M+%7D+%7B+%5Csqrt+%7B+d+_+%7B+k+%7D+%7D+%7D+%5Cright%29+%7D+%5C%5C+%7B+h+%5E+%7B+i+%2B+1+%7D+%3D+S+%5E+%7B+i+%2B+1+%7D+V+%5E+%7B+i+%2B+1+%7D+%7D+%5Cend%7Barray%7D+" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_b.jpg" alt="img"></p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">前一篇</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
    <a href="../AAAI22/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">后一篇</strong>
      <div class="article-nav-title">
        
          AAAI22 实验全记录
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Zhaojiangwei<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a><br>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="../../../../index.html" class="mobile-nav-link">Home</a>
  
    <a href="../../../../archives" class="mobile-nav-link">Archives</a>
  
    <a href="../../../../about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="../../../../js/jquery-1.4.3.min.js"></script>


  
<script src="../../../../fancybox/jquery.fancybox-1.3.4.js"></script>




<script src="../../../../js/script.js"></script>






<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </div>
</body>
</html>