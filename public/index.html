<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>赵江伟的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
    
<link rel="stylesheet" href="fancybox/jquery.fancybox-1.3.4.css">

  
  
<link rel="stylesheet" href="css/style.css">

  <meta name="google-site-verification" content="s7zZsgTqZwqYOoqReZl1ZE6FOOsSN0slhQFB9RTy0ag" />
  <meta name="baidu-site-verification" content="code-yqz8yGm4Fd" />
<meta name="generator" content="Hexo 5.4.2"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="index.html" id="logo">赵江伟的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="index.html">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-关于《中共中央关于党的百年奋斗重大成就和历史经验的决议》的说明" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/%E5%85%B3%E4%BA%8E%E3%80%8A%E4%B8%AD%E5%85%B1%E4%B8%AD%E5%A4%AE%E5%85%B3%E4%BA%8E%E5%85%9A%E7%9A%84%E7%99%BE%E5%B9%B4%E5%A5%8B%E6%96%97%E9%87%8D%E5%A4%A7%E6%88%90%E5%B0%B1%E5%92%8C%E5%8E%86%E5%8F%B2%E7%BB%8F%E9%AA%8C%E7%9A%84%E5%86%B3%E8%AE%AE%E3%80%8B%E7%9A%84%E8%AF%B4%E6%98%8E/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.645Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E6%94%BF%E6%B2%BB/">政治</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2022/11/03/%E5%85%B3%E4%BA%8E%E3%80%8A%E4%B8%AD%E5%85%B1%E4%B8%AD%E5%A4%AE%E5%85%B3%E4%BA%8E%E5%85%9A%E7%9A%84%E7%99%BE%E5%B9%B4%E5%A5%8B%E6%96%97%E9%87%8D%E5%A4%A7%E6%88%90%E5%B0%B1%E5%92%8C%E5%8E%86%E5%8F%B2%E7%BB%8F%E9%AA%8C%E7%9A%84%E5%86%B3%E8%AE%AE%E3%80%8B%E7%9A%84%E8%AF%B4%E6%98%8E/">关于《中共中央关于党的百年奋斗重大成就和历史经验的决议》的说明</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>习近平</p>
<p>受中央政治局委托, 我就《中共中央关于党的百年奋斗重大成就和历史经验的决议》起草的有关情况向全会作说明.</p>
        
          <p class="article-more-link">
            <a href="2022/11/03/%E5%85%B3%E4%BA%8E%E3%80%8A%E4%B8%AD%E5%85%B1%E4%B8%AD%E5%A4%AE%E5%85%B3%E4%BA%8E%E5%85%9A%E7%9A%84%E7%99%BE%E5%B9%B4%E5%A5%8B%E6%96%97%E9%87%8D%E5%A4%A7%E6%88%90%E5%B0%B1%E5%92%8C%E5%8E%86%E5%8F%B2%E7%BB%8F%E9%AA%8C%E7%9A%84%E5%86%B3%E8%AE%AE%E3%80%8B%E7%9A%84%E8%AF%B4%E6%98%8E/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E6%94%BF%E6%B2%BB/" rel="tag">政治</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-ViTGAN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/ViTGAN/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.636Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="vitgan:-training gans with vision transformers">ViTGAN: Training GANs with Vision Transformers</h1>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/11/215330-882056.png" alt="image-20211211215328705"></p>
<h2 id="abstract">Abstract</h2>
<p>最近, 视觉变形(ViTs)在图像识别方面表现出了有竞争力的性能, 同时需要较少的视觉特异性诱导偏差. 在本文中, 我们研究了这种观测是否可以扩展到图像生成. 为此, 我们将ViT架构整合到生成式对抗网络(GANs)中. 我们观察到现有的GANs正则化方法与自我注意交互不良, 导致训练过程中严重的不稳定性. 为了解决这个问题, 我们引入了一种新的正则化技术来用vit训练gan. 根据经验, 我们的方法名为vitgan, 在CIFAR-10、CelebA和LSUN数据集上实现了与最先进的基于cnn的StyleGAN2相当的性能.</p>
<h2 id="related-work">Related Work</h2>
<h3 id="vision-transformers">Vision Transformers</h3>
<p>视觉变压器(ViT)是一个无卷积变压器, 执行图像分类序列的图像补丁. ViT通过对大规模数据集进行预训练, 展示了Transformer架构相对于传统cnn的优势. 随后, DeiT通过知识蒸馏和正则化技巧提高了vit的采样效率. MLP- mixer进一步drop自我注意, 并取代它由一个MLP混合每位置的功能. 我们的工作是第一批在GAN模型中利用视觉变形进行图像生成.</p>
<h3 id="generative-transformer in vision">Generative Transformer in Vision</h3>
<p>在GPT-3成功的激励下, 一些试点工作使用Transformer通过自回归学习或图像和文本之间的跨模态学习来研究图像生成. 这些方法与我们的不同, 因为它们将图像生成建模为一个自回归序列学习问题. 相反, 我们的工作是在生成对抗训练范式中训练视觉变形金刚. 最接近我们的作品是TransGAN, 呈现了一个基于GAN模型的纯变压器. 在提出多任务联合训练和局部初始化以获得更好的训练时, TransGAN忽略了训练稳定性的关键技术, 表现远远落后于领先的卷积GAN模型. 通过我们的设计, 本文首次证明了基于transformer的GAN能够实现与最先进的基于cnn的GAN模型相比的具有竞争力的性能.</p>
<h2 id="preliminaries:-vision transformers (vits)">Preliminaries: Vision Transformers (ViTs)</h2>
<ul>
<li>
<p>2D image <span class="markdown-them-math-inline">$x\in \Bbb{R}^{H\times W\times C}$</span> flatten to a sequence of image patchs <span class="markdown-them-math-inline">$x_p\in \Bbb{R}^{L\times(P^2C)}$</span>  where <span class="markdown-them-math-inline">$L=\frac{H\times W}{P^2}$</span></p>
</li>
<li>
<p>Following BERT, add classification embedding <span class="markdown-them-math-inline">$x_{class}$</span> to image squence , with added 1D positional embeddings <span class="markdown-them-math-inline">$E_{pos}$</span> to formulate patch embedding <span class="markdown-them-math-inline">$h_0$</span></p>
</li>
<li>
<p>architecture:</p>
<ul>
<li>
<div class="markdown-them-math-block">$$h_0=[x_{class};x_p^1E;x_p^2E;...;x_p^LE]+E_{pos},\\E\in \Bbb{R}^{(P^2C)\times D},E_{pos}\in \Bbb{R}^{(L+1)\times D}
$$</div></li>
<li>
<div class="markdown-them-math-block">$$h_l'=MSA(LN(h_{l-1}))+h_{l-1},l=1,...,L
$$</div></li>
<li>
<div class="markdown-them-math-block">$$h_l=MLP(LN(h_l'))+h_l',l=1,...,L
$$</div></li>
<li>
<div class="markdown-them-math-block">$$y=LN(h_0)
$$</div></li>
</ul>
</li>
<li>
<p>其中注意力部分</p>
<ul>
<li>
<div class="markdown-them-math-block">$$Attention_h(X)=softmax(\frac{QK^T}{\sqrt{d_h}})V
$$</div></li>
<li>
<div class="markdown-them-math-block">$$MSA(X)=concat_{h=1}^H[Attention_h(X)]W+b
$$</div></li>
</ul>
</li>
</ul>
<h2 id="method">Method</h2>
<h3 id="regularizing-vit-based discriminator">Regularizing ViT-based discriminator</h3>
<h4 id="enforcing-lipschitzness of transformer discriminator">Enforcing Lipschitzness of Transformer Discriminator</h4>
<p>最近的工作表明, 标注的点积自我注意力层的Lipschitz常数可以是无界的, 在vit中违反了Lipschitz连续性. 为了加强我们的ViT鉴别器的Lipschitzness, 我们将点积相似度替换为欧氏距离, 并将self-attention的query和key的投影矩阵的权值绑在一起:</p>
<div class="markdown-them-math-block">$$Attention_h(X)=softmax(\frac{d(XW_q,XW_k)}{\sqrt{d_h}})XW_v,where W_q=W_k
$$</div><h4 id="improved-spectral normalization">Improved Spectral Normalization.</h4>
<p>我们发现变压器块对李普希茨常数的尺度很敏感, 当使用SN时, 训练进展非常缓慢. 类似地, 我们发现R1梯度惩罚在使用基于vitc的鉴别器时削弱GAN训练. [14]表明MLP块的小Lipschitz常数可能导致Transformer的输出压缩为秩1矩阵. 为了解决这个问题, 我们建议增加投影矩阵的谱范数. 我们发现, 将每一层的归一化权矩阵与谱范数在初始化时相乘就足以解决这一问题.</p>
<div class="markdown-them-math-block">$$\bar{W}_{ISN}(W):=\sigma(W_{init})W/\sigma(W)
$$</div><h4 id="overlapping-image patches">Overlapping Image Patches</h4>
<p>ViT判别器由于其学习能力较强, 容易出现过拟合. 我们的判别器和生成器使用相同的图像表示, 根据预定义的网格 P×P将图像划分为不重叠的补丁序列. 这些任意的网格分区, 如果不仔细调整, 可能会鼓励判别器记住局部线索, 并停止为生成器提供有意义的损失. 我们使用一个简单的技巧来缓解这个问题, 通过允许图像补丁之间的一些重叠. 对于patch的每个边界边缘, 我们通过像素来扩展它, 使有效的patch大小<span class="markdown-them-math-inline">$(P+ 2o)×(P+ 2o)$</span>.</p>
<h3 id="generator-design">Generator Design</h3>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/12/155142-11132.png" alt="Generator Architectures."></p>
<div class="markdown-them-math-block">$$h_0=E_{pos},E_{pos}\in \Bbb{E}^{L\times D}
$$</div><div class="markdown-them-math-block">$$h_l'=MSA(SLN(h_{l-1},w))+h_{l-1}, l=1,...,L,w\in \Bbb{R}^D
$$</div><div class="markdown-them-math-block">$$h_l=MLP(SLN(h_l',w))+h_l', l=1,...,L
$$</div><div class="markdown-them-math-block">$$y=SLN(h_L,w)=[y^1,...,y^L], y^1,...,y^L\in \Bbb{R}^D
$$</div><div class="markdown-them-math-block">$$x=[x_p^1,...,x_p^L]=[f_\theta(E_{fou},y^1),...,f_\theta(E_{fou},y^L)],  x_p^i\in \Bbb{R}^{P^2\times C},x\in \Bbb{R}^{H\times W\times C}
$$</div><h4 id="self-modulated-layernorm">Self-modulated LayerNorm</h4>
<div class="markdown-them-math-block">$$\operatorname{SLN}\left(\mathbf{h}_{\ell}, \mathbf{w}\right)=\operatorname{SLN}\left(\mathbf{h}_{\ell}, \operatorname{MLP}(\mathbf{z})\right)=\gamma_{\ell}(\mathbf{w}) \odot \frac{\mathbf{h}_{\ell}-\boldsymbol{\mu}}{\sigma}+\beta_{\ell}(\mathbf{w}),
$$</div><h4 id="implicit-neural representation for patch generation">Implicit Neural Representation for Patch Generation</h4>
<p>我们使用一种隐式神经表示来学习一个patch embeddding <span class="markdown-them-math-inline">$y^i\in \Bbb{R}^D$</span>to patch pixel values <span class="markdown-them-math-inline">$x^i_p\in \Bbb{R}^{P^2\times C}$</span>的连续映射.</p>
<p>当结合傅里叶特征[49]或正弦激活函数[46]时, 隐式表示可以将生成样本的空间限制为光滑变化的自然信号的空间.</p>
<p>其中<span class="markdown-them-math-inline">$E_{fou}$</span>为P×P位置的傅里叶编码 , <span class="markdown-them-math-inline">$f_\theta(.,.)$</span> 为2-layer MLP</p>
<p>值得注意的是, 产生器和判别器可以有不同的图像网格, 从而有不同的序列长度. 我们发现, 当将我们的模型缩放到更高分辨率的图像时, 通常只增加鉴别器的序列长度或特征维数就足够了.</p>
<h2 id="experiments">Experiments</h2>
<h3 id="implementation-details">Implementation Details</h3>
<p>对于32×32分辨率, 我们使用一个4块基于维特的鉴别器和一个4块基于ViT-GAN的生成器. 对于64×64分辨率, 我们将块的数量增加到6. vi - small[15]之后, 所有Transformer块的输入/输出特征维数为384, 而MLP隐藏维数为1536. 不像[15], 我们选择了6个注意力头. 我们发现增加头数并不能改善GAN训练. 对于32×32分辨率, 我们使用补丁大小4×4, 生成64个补丁的序列长度. 对于64×64分辨率, 我们只需将补丁大小增加到8×8, 并保持与32×32分辨率中的序列长度相同. 平移, 颜色, 切割, 缩放数据扩展[59,25]的应用概率为0.8. 所有基于基线变压器的GAN模型, 包括我们的, 使用平衡一致性正则化(bCR),  <span class="markdown-them-math-inline">$λ_{real}=λ_{fake}= 10.0$</span>. 除了bCR, 我们不使用正则化方法通常用于训练vit[51], 如Dropout, weight decay, 或Stochastic Depth. 我们发现, LeCam正则化[53], 类似于bCR, 提高了性能. 但为了更清晰的消融, 我们不包括LeCam正则化. 我们用Adam来训练我们的模型, <span class="markdown-them-math-inline">$β1= 0.0,  β2= 0.99$</span>, [27]练习后的学习率为0.002. 此外, 我们采用非饱和logistic损失[18], 指数滑动平均[24]和均衡学习率[24]. 我们使用128个小批量. ViTGAN和StyleGAN2都是基于Tensorflow 2实现[36]. 我们在谷歌云TPU v2-32和v3-8上训练我们的模型.</p>
<h3 id="main-results">Main Results</h3>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/12/160943-984006.png" alt="image-20211212160940538"></p>
<h3 id="ablation-studies">Ablation Studies</h3>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/12/161114-817972.png" alt="image-20211212161113820"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/12/161146-870490.png" alt="image-20211212161146063"></p>
<h2 id="more-quantitative results">More Quantitative Results</h2>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/12/161308-846160.png" alt="image-20211212161308284"></p>
<h2 id="implementation-notes">Implementation Notes</h2>
<h3 id="patch-extraction">Patch Extraction</h3>
<p>我们使用了一个简单的技巧, 通过允许图像补丁之间的一些重叠来减轻基于维特的鉴别器的过拟合. 对于patch的每个边界边缘, 我们将其扩展为o像素, 使有效的patch大小(P+ 2o)×(P+ 2o), 其中o= p/2. 虽然这个操作与一个具有核(P+ 2o)×(P+ 2o)和strideP×P的卷积操作有联系, 但由于在我们的实现中没有使用卷积, 所以在我们的模型中并没有将其视为卷积算子. 注意, V anilla ViT[15]中的(非重叠)补丁的提取也与kernelP×Pand strideP×P的卷积操作有连接.</p>
<h3 id="positional-embedding">Positional Embedding</h3>
<p>ViT网络的每一个位置嵌入都是一个贴片位置的线性投影, 后面跟着一个正弦激活函数. patch的位置被标准化到−1.0和1.0之间.</p>
<h3 id="implicit-neural representation for patch generation-1">Implicit Neural Representation for Patch Generation</h3>
<p>每个位置嵌入都是像素坐标的线性投影, 后面跟着一个正弦激活函数(因此称为傅里叶编码). <span class="markdown-them-math-inline">${P^2}$</span>像素的像素坐标被标准化到−1.0和1.0之间. 2层MLP将位置嵌入<span class="markdown-them-math-inline">$E_{fou}$</span>作为其输入, 并以[27,1]中通过权重调制的patch嵌入为条件.</p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-knowledge_bart" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/knowledge_bart/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.625Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="bart">BART</h1>
<p>BART是Bidirectional and Auto-Regressive Transformers的简写, 来自论文: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1910.13461.pdf">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a></p>
<h2 id="背景:-seq2seq预训练">背景: Seq2Seq预训练</h2>
<p>去年10月, 来自Google和Facebook的团队分别发布了新的Transformer-related论文: T5和BART.  这两篇论文在如抽象总结和对话等生成任务上都取得了更好的下游性能, 主要有两个改变:</p>
<ul>
<li>在BERT的双向编码器架构中添加因果解码器;</li>
<li>用更复杂的预训练任务代替BERT的完形填空任务.</li>
</ul>
<h3 id="**bert-vs gpt2**"><strong>Bert vs. GPT2</strong></h3>
<p>正如BART作者在论文中写的,</p>
<blockquote>
<p>(BART) can be seen as generalizing Bert (due to the bidirectional encoder) and GPT2 (with the left to right decoder).</p>
</blockquote>
<h3 id="**bert**"><strong>BERT</strong></h3>
<p>ERT最重要的预训练任务是预测masked token, 并使用整个输入来获取更完全的信息以进行更准确的预测. 这对于那些允许利用位置<span class="markdown-them-math-inline">$i$</span>之后的信息来预测位置<span class="markdown-them-math-inline">$i$</span>的任务是有效的, 但是对于诸如文本生成之类的任务则没有多大用处, 这些对位置<span class="markdown-them-math-inline">$i$</span>的预测只能取决于先前生成的单词.</p>
<p>在BERT源码中, 在预测位置<span class="markdown-them-math-inline">$i$</span>时可以使用哪些信息是由由一个称为<code>attention_mask</code>的参数来控制的,  注意掩码中的值为1表示模型在预测行的单词时可以利用的列单词的信息.</p>
<p>下图是BERT的&quot;Fully-visible&quot; 注意力矩阵,</p>
<p><img src="https://pic1.zhimg.com/80/v2-f964940fa1025254317a087cfb27bb0c_1440w.jpg" alt="img"></p>
<ul>
<li>
<p><img src="https://pic1.zhimg.com/80/v2-ac5de7890849432e16681a881fea5e50_1440w.png" alt="img"></p>
</li>
<li>
<p>优点: 本质为降噪自编码特征表示, 通过引入噪声[MASK]构建MLM, 获取上下文相关的双向特征表示;</p>
</li>
<li>
<p>引入独立性假设, 为<strong>联合概率的有偏估计</strong>, 没有考虑预测[MASK]之间的相关性</p>
</li>
<li>
<ul>
<li>不适合直接处理生成任务, MLM预训练目标的设置造成预训练过程和生成过程不一致;</li>
<li>预训练时的[MASK]噪声在finetune阶段不会出现, 造成两阶段不匹配问题;</li>
</ul>
</li>
<li>
<p>代表模型: BERT系列模型;</p>
</li>
</ul>
<h3 id="**gpt**"><strong>GPT</strong></h3>
<p>GPT预训练任务使用的是<code>autoregressive</code>的思想, 使用已经解码出的信息来预测下一个位置. 该种模式对于生成任务更为有效, 而对于那些可以使用全局输入来得到输出的下游任务则比较差劲.</p>
<p>同样的, 给出GPT的注意力矩阵,</p>
<p><img src="https://pic1.zhimg.com/80/v2-bdf9db30fdceb59a9d3d3a07c1a67300_1440w.jpg" alt="img"></p>
<p>在这里, 当我们预测<code>eating</code>时, 可以使用的信息只有<code>&lt;BOS&gt; I love</code>.</p>
<p><img src="https://pic3.zhimg.com/80/v2-4f6492d8b4303d278441bbb4c933a20a_1440w.png" alt="img"></p>
<ul>
<li>
<p>优点:</p>
</li>
<li>
<ul>
<li>文本序列<strong>联合概率的密度估计</strong>, 即为传统的语言模型, 天然适合处理自然生成任务;</li>
</ul>
</li>
<li>
<p>缺点:</p>
</li>
<li>
<ul>
<li>联合概率按照文本序列从左至右分解 (<strong>顺序拆解</strong>) , 无法通过上下文信息进行双向特征表征;</li>
</ul>
</li>
<li>
<p>代表模型: ELMO/GPT1.0/GPT2.0;</p>
</li>
<li>
<p>改进: XLNet将传统的自回归语言模型进行推广, 将顺序拆解变为<strong>随机拆解</strong> (排列语言模型) , 产生上下文相关的双向特征表示;</p>
</li>
</ul>
<h3 id="**encoder-decoder**"><strong>Encoder-Decoder</strong></h3>
<p>我们的新朋友, 例如BART, 可以做到两全其美.</p>
<p>其中Encoder的注意力矩阵是<code>Fully-visible</code>的,</p>
<p><img src="https://pic1.zhimg.com/80/v2-f964940fa1025254317a087cfb27bb0c_1440w.jpg" alt="img"></p>
<p>而Decoder的注意力矩阵是<code>autoregressive</code>,</p>
<p><img src="https://pic3.zhimg.com/80/v2-c1c1051fa14cc3641c0c764f0cad5b62_1440w.jpg" alt="img"></p>
<p>编码器和解码器通过<code>cross attention</code>连接, 其中每个解码器层都对编码器输出的最终隐藏状态进行attention操作, 这会使得模型生成与原始输入紧密相关的输出.</p>
<h3 id="transformer">transformer</h3>
<p><img src="https://s1.ax1x.com/2020/04/25/JyCdy9.png" alt="img"></p>
<h2 id="pre-training">Pre-training</h2>
<ul>
<li><strong>Token Masking</strong>: Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with<code>[MASK]</code>elements.</li>
<li><strong>Sentence Permutation</strong>: A document is divided into sentences based on full stops, and these sentences are shuffled in a random order.</li>
<li><strong>Document Rotation</strong>: A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document.</li>
<li><strong>Token Deletion</strong>: Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs.</li>
<li><strong>Text Infilling</strong>: A number of text spans are sampled, with span lengths drawn from a Poisson distribution (λ=3). Each span is replaced with a single<code>[MASK]</code>token. 0-length spans correspond to the insertion of <code>[MASK]</code> tokens. Text infilling teaches the model to predict how many tokens are missing from a span.</li>
</ul>
<p><img src="https://z3.ax1x.com/2021/04/06/c18o11.png#shadow" alt="img"></p>
<h2 id="**fine-tuning**"><strong>Fine-tuning</strong></h2>
<h3 id="sequence-classification tasks">Sequence Classification Tasks</h3>
<p>序列分类任务中, 编码器和解码器的输入相同, 解码器 token 的最终隐藏状态被输入到多类别线性分类器中. BART 在解码器最后额外添加了一个 token, 如下图所示, 该 token 位置的输出可以被认为是该句子的 representation</p>
<p><img src="https://pic2.zhimg.com/80/v2-869ae8f65a9f1be753a51ee2f9011f31_1440w.jpg" alt="img">在上述示例中, 原始文档为<code>A B C D E</code>. 在编码之前将文本<code>[C, D]</code>屏蔽掉, 又在B之前插入一个额外的掩码, 然后将损坏的文档<code>A _ B _ E</code>作为编码器的输入. 解码器必须使用编码器的输出和先前未损坏的标记来重建原始文档.</p>
<h3 id="sequence-generation tasks">Sequence Generation Tasks</h3>
<p>由于 BART 具备自回归解码器, 因此它可以针对序列生成任务进行直接微调, 如问答或者文本摘要</p>
<h3 id="machine-translation">Machine Translation</h3>
<p>作者采用新的随机初始化 Encoder 替换 BART 编码器的 <code>Embedding </code>层. 该模型以端到端的方式进行训练, 即训练一个新的编码器将外来词映射到输入. 新的编码器可以使用不同于原始 BART 模型的词汇. 其中随机初始化 Encoder 的训练分两步, 均需要将来自 BART 模型输出的交叉熵损失进行反向传播. 第一步, 作者冻结 BART 的大部分参数, 仅更新随机初始化的 Encoder、BART 位置嵌入和 BART 编码器第一层的自注意力输入投影矩阵. 第二步, 作者将所有模型参数进行少量迭代训练</p>
<p><img src="https://z3.ax1x.com/2021/04/06/c1GPnf.png" alt="img"></p>
<h2 id="**results**"><strong>Results</strong></h2>
<p><img src="https://z3.ax1x.com/2021/04/06/c1GFHS.png" alt="img"></p>
<p>从上表可以看出, 貌似带上 Document Rotation 或 Sentence Shuffling 效果都不是太好, 可以这么理解, 假如模型在训练的时候看到的句子顺序都是乱的, 它可能就认为这个世界的句子顺序都是乱的, 当你做测试的时候, 输入的句子是正序的, 可能模型就不知所措了. 实际上 Text Infilling 可以看作是 Token Masking+Token Deletion, 所以 Text Infilling 效果这么好也可以理解</p>
<p><img src="https://z3.ax1x.com/2021/04/06/c1Guj0.png" alt="img"></p>
<h2 id="应用">应用</h2>
<h2 id="km-bart:-knowledge enhanced multimodal bart for visual commonsense generation">KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation</h2>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/23/203112-858339.png" alt="image-20211223202316964"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.00419.pdf">2101.00419.pdf (arxiv.org)</a></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/23/202524-103198.png" alt="image-20211223202500339"></p>
<p>可以看到, 整个模型的backbone就是一个BART, 为了能适配多模态输入, 论文通过在原始的BART中增加特殊的tokens来适应不同的预训练任务. 在编码器部分, 对于不同的预训练任务, 通过加入几个特殊tokens来区分不同模态的输入:</p>
<ul>
<li>常识生成: <code>&lt;before&gt;</code>, <code>&lt;after&gt;</code>和<code>&lt;intent&gt;</code></li>
<li>属性和关系预测: <code>&lt;region_caption&gt;</code></li>
<li>掩码预训练语言模型: <code>&lt;caption&gt;</code></li>
</ul>
<p>对于不同模态的输入, 引入几个特殊tokens:</p>
<ul>
<li>视觉embedding: <code>&lt;img&gt;</code>, <code>&lt;/img&gt;</code></li>
<li>事件文本: <code>&lt;event&gt;</code>, <code>&lt;/event&gt;</code></li>
<li>语言模型: <code>&lt;mlm&gt;</code>, <code>&lt;/mlm&gt;</code></li>
</ul>
<p>解码器利用类似于GPT的单向Transformer, 模型需要预测被masked的单词和被masked视觉区域的类别分布.</p>
<p>这项研究中到底哪里用了知识图谱? 其实就在于其中一个预训练任务基于知识的常识生成任务, 这个预训练任务使用了COMET, 这是一个在常识知识图谱上得到的预训练语言模型, 所以可以看到这篇论文所谓的视觉加知识图谱其实就是在知识融入的PLM中调整输入tokens保证可以编码视觉输入数据.</p>
<h3 id=""></h3>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/hello-world/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.623Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/test/">test</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2022/11/03/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
        
          <p class="article-more-link">
            <a href="2022/11/03/hello-world/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/test/" rel="tag">test</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-bert+knowledge" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/bert+knowledge/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.619Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="**[kg-bert:-bert for knowledge graph completion(2019)](https://linkzhihu.com/?target=https%3a//arxiv.org/abs/1909.03193)**"><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.03193">KG-BERT: BERT for Knowledge Graph Completion(2019)</a></strong></h2>
<p>这篇文章是介绍知识库补全方面的工作, 结合预训练模型BERT可以将更丰富的上下文表示结合进模型中, 在三元组分类、<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=%E9%93%BE%E6%8E%A5%E9%A2%84%E6%B5%8B&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">链接预测</a>以及关系预测等任务中达到了SOTA效果.</p>
<p>具体的做法也非常简单易懂, 就是修改了BERT模型的输入使其适用于知识库三元组的形式.</p>
<p><img src="https://pic1.zhimg.com/v2-dd8cf5259b8ff15120b7908824353e28_b.jpg" alt="img">首先是<strong>KG-BERT(a)</strong>, 输入为三元组 <span class="markdown-them-math-inline">$(h,r,t)$</span>的形式, 当然还有BERT自带的special tokens. 举个栗子, 对于三元组  <span class="markdown-them-math-inline">$(SteveJobs,foouned,AppleInc)$</span>, 上图中的<code>Head Entity</code>输入可以表示为<code>Steven Paul Jobs was an American business magnate, entrepreneur and investor</code>或者<code>Steve Jobs</code>, 而<code>Tail Entity</code>可以表示为<code>Apple Inc. is an American multinational technology company headquartered in Cupertino, California</code>或<code>Apple Inc</code>. 也就是说, 头尾实体的输入可以是<strong>实体描述</strong>句子或者<strong>实体名</strong>本身.</p>
<p>模型训练是首先分别构建**<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=positive+triple+set&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">positive triple set</a><strong>和</strong>negative triple set**, 然后用BERT的[CLS]标签做一个sigmoid打分以及最后交叉熵损失</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%2B%5Ccup+%5Cmathbb%7BD%7D%5E%7B-%7D%7D%5Cleft%28y_%7B%5Ctau%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+0%7D%5Cright%29%2B%5Cleft%281-y_%7B%5Ctau%7D%5Cright%29+%5Clog+%5Cleft%28s_%7B%5Ctau+1%7D%5Cright%29%5Cright%29" alt="[公式]"></p>
<p><img src="https://pic3.zhimg.com/v2-fc24b1e3a98066bd914fe54af731f846_b.jpg" alt="img"></p>
<p>上述的<strong>KG-BERT(a)<strong>需要输入关系, 对于关系分类任务不适用, 于是作者又提出一种</strong>KG-BERT(b)</strong>, 如上图. 这里只是把sigmoid的二分类改成了softmax的关系多分类.</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%5E%7B%5Cprime%7D%3D-%5Csum_%7B%5Ctau+%5Cin+%5Cmathbb%7BD%7D%5E%7B%2B%7D%7D+%5Csum_%7Bi%3D1%7D%5E%7BR%7D+y_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D+%5Clog+%5Cleft%28s_%7B%5Ctau+i%7D%5E%7B%5Cprime%7D%5Cright%29" alt="[公式]"></p>
<h2 id="**[k-bert:-enabling language representation with knowledge graph(2019)](https://linkzhihu.com/?target=https%3a//arxiv.org/abs/1909.07606)**"><strong><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1909.07606">K-BERT: Enabling Language Representation with Knowledge Graph(2019)</a></strong></h2>
<p>作者指出通过公开语料训练的BERT模型仅仅是获得了general knowledge, 就像是一个普通人, 当面对特定领域的情境时 (如医疗、金融等) , 往往表现不如意, 即<strong>domain discrepancy</strong>. 而本文提出的<strong>K-BERT</strong>则像是领域专家, 通过将知识库中的结构化信息 (三元组) 融入到预训练模型中, 可以更好地解决领域相关任务. 如何将外部知识整合到模型中成了一个关键点, 这一步通常存在两个难点:</p>
<ul>
<li>**Heterogeneous Embedding Space: ** 即文本的单词embedding和知识库的实体实体embedding通常是通过不同方式获取的, 使得他俩的向量空间不一致;</li>
<li>**Knowledge Noise: ** 即过多的知识融合可能会使原始句子偏离正确的本意, 过犹不及.</li>
</ul>
<p>模型的整体框架如下图, 主要包括了四个子模块:  <strong>knowledge layer</strong>, <strong>embedding layer</strong>, <strong>seeing layer</strong> 和 <strong>mask-transformer</strong>. 对于一个给定的输入 <span class="markdown-them-math-inline">$s={w_0,w_1,w_2,...,w_n}$</span>, 首先 <strong>knowledge layer</strong>会从一个KG中注入相关的三元组, 将原来的句子转换成一个knowledge-rich的句子树; 接着句子树被同时送入<strong>embedding layer</strong>和<strong>seeing layer</strong>生成一个token级别的embedding表示和一个可见矩阵 (visible matrix); 最后通过<strong>mask-transformer</strong>层编码后用于下游任务的输出.</p>
<p><img src="https://pic4.zhimg.com/v2-bf6449f45518cf3f88af7a62affb0247_b.jpg" alt="img"></p>
<h3 id="**knowledge-layer**"><strong>Knowledge Layer</strong></h3>
<p>这一层的输入是原始句子 <span class="markdown-them-math-inline">$s=\{w_0,w_1,w_2,...,w_0\}$</span> , 输出是融入KG信息后的句子树<span class="markdown-them-math-inline">$t=\{w_0,w_1,...,w_i\{(r_{i,0},w_{i,0},...,r_{i,k},w_{i,k})\},...w_n\}$</span></p>
<p>通过两步完成:</p>
<ul>
<li><strong>K-Query</strong> 输入句子中涉及的所有实体都被选中, 并查询它们在KG中对应的三元组 <span class="markdown-them-math-inline">$E$</span> ;</li>
<li><strong>K-Inject</strong> 将查询到的三元组注入到句子<span class="markdown-them-math-inline">$S$</span> 中, 将  <span class="markdown-them-math-inline">$E$</span>中的三元组插入到它们相应的位置, 并生成一个句子树  <span class="markdown-them-math-inline">$t$</span>.</li>
</ul>
<h3 id="**embedding-layer**"><strong>Embedding Layer</strong></h3>
<p>K-BERT的输入和原始BERT的输入形式是一样的, 都需要token embedding, position embedding和segment embedding, 不同的是, K-BERT的输入是一个句子树, 因此问题就变成了句子树到序列化句子的转化, 并同时保留结构化信息.</p>
<p><img src="https://pic4.zhimg.com/v2-d1a6c85faf6a3db43125f56e96ac672b_b.jpg" alt="img"><strong>Token embedding</strong></p>
<p>句子树的序列化, 作者提出一种简单的重排策略: <strong>分支中的token被插入到相应节点之后, 而后续的token被向后移动</strong>. 举个栗子, 对于上图中的句子树, 则重排后变成了<code>Tim Cook CEO Apple is visiting Beijing capital China is a City now</code>. 没错, 这的确看上去毫无逻辑, 但是还好后面可以通过trick来解决.</p>
<h3 id="**soft-position-embedding**"><strong>Soft-position embedding</strong></h3>
<p>通过重排后的句子显然是毫无意义的, 这里利用了position embedding来还原回结构信息. 还是以上图为例, 重排后, <code>CEO</code>和<code>Apple</code>被插入在了<code>Cook</code>和<code>is</code>之间, 但是<code>is</code>应该是接在<code>Cook</code>之后一个位置的, 那么我们直接把<code>is</code>的position number 设置为3即可. Segment embedding 部分同BERT一样.</p>
<h3 id="**seeing-layer**"><strong>Seeing Layer</strong></h3>
<p>作者认为Seeing layer的mask matrix是K-BERT有效的关键, 主要解决了前面提到的<strong>Knowledge Noise</strong>问题. 栗子中<code>China</code>仅仅修饰的是<code>Beijing</code>, 和<code>Apple</code>半毛钱关系没有, 因此像这种token之间就不应该有相互影响. 为此定义一个可见矩阵, 判断句子中的单词之间是否彼此影响</p>
<p><img src="https://www.zhihu.com/equation?tex=M_%7Bi+j%7D%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bcc%7D%7B0%7D%26%7Bw_%7Bi%7D+%5Cominus+w_%7Bj%7D%7D+%5C%5C+%7B-%5Cinfty%7D+%26+%7Bw_%7Bi%7D+%5Coslash+w_%7Bj%7D%7D%5Cend%7Barray%7D%5Cright." alt="[公式]"></p>
<h3 id="**mask-transformer**"><strong>Mask-Transformer</strong></h3>
<p>BERT中的Transformer Encoder不能接受上述可见矩阵作为输入, 因此需要稍作改进. Mask-Transformer是一层层<a target="_blank" rel="noopener" href="https://www.zhihu.com/search?q=mask-self-attention&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22article%22%2C%22sourceId%22%3A%2291052495%22%7D">mask-self-attention</a>的堆叠,</p>
<p><img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Barray%7D+%7B+c+%7D+%7B+Q+%5E+%7B+i+%2B+1+%7D+%2C+K+%5E+%7B+i+%2B+1+%7D+%2C+V+%5E+%7B+i+%2B+1+%7D+%3D+h+%5E+%7B+i+%7D+W+_+%7B+q+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+k+%7D+%2C+h+%5E+%7B+i+%7D+W+_+%7B+v+%7D+%7D+%5C%5C+%7B+S+%5E+%7B+i+%2B+1+%7D+%3D+%5Coperatorname+%7B+softmax+%7D+%5Cleft%28+%5Cfrac+%7B+Q+%5E+%7B+i+%2B+1+%7D+K+%5E+%7B+i+%2B+1+%7D+%2B+M+%7D+%7B+%5Csqrt+%7B+d+_+%7B+k+%7D+%7D+%7D+%5Cright%29+%7D+%5C%5C+%7B+h+%5E+%7B+i+%2B+1+%7D+%3D+S+%5E+%7B+i+%2B+1+%7D+V+%5E+%7B+i+%2B+1+%7D+%7D+%5Cend%7Barray%7D+" alt="[公式]"></p>
<p><img src="https://pic4.zhimg.com/v2-4689a5cd50175ec3a36e901b8bb6b733_b.jpg" alt="img"></p>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-AAAI22" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2022/11/03/AAAI22/" class="article-date">
  <time class="dt-published" datetime="2022-11-03T03:27:06.615Z" itemprop="datePublished">2022-11-03</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E6%AF%94%E8%B5%9B/">比赛</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2022/11/03/AAAI22/">AAAI22 实验全记录</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="比赛题目">比赛题目</h2>
<p><a target="_blank" rel="noopener" href="https://tianchi.aliyun.com/competition/entrance/531939/introduction?spm=5176.12281957.1004.1.26703eafcBnEKj">AAAI-2022 安全AI挑战者计划第八期: 以数据为中心的鲁棒机器学习-天池大赛-阿里云天池 (aliyun.com)</a></p>
<p>提交模版示例也可参考: <a target="_blank" rel="noopener" href="https://github.com/vtddggg/training_template_for_AI_challenger_sea8">https://github.com/vtddggg/training_template_for_AI_challenger_sea8</a></p>
<h3 id="介绍">介绍</h3>
<p>当前的机器学习竞赛主要是基于固定的数据集去追求一个高性能的机器学习模型, 而最近的以数据为中心的人工智能竞赛 (<a target="_blank" rel="noopener" href="https://https-deeplearning-ai.github.io/data-centric-comp/">https://https-deeplearning-ai.github.io/data-centric-comp/</a>)改变了传统范式, 即给定固定模型旨在去改进数据集. 类似地, 在鲁棒学习方面, 已经提出了基于深度学习模型的防御方法来减轻对抗样本的潜在威胁, 但大多数方法都是在固定的约束和数据集下去追求高性能模型. 因此, 目前尚未广泛探索如何构建通用且有效的数据集来训练鲁棒模型.</p>
<p>在图像分类的对抗鲁棒性研究中, 为了加快以数据为中心的相关的技术研究, 我们组织了本次比赛, 目的是开发新的以数据为中心的算法, 例如数据增强、标签细化、制造对抗性数据, 甚至设计来自其他领域的知识融合算法. 鼓励参与者自由开发新颖的想法, 找到有效的以数据为中心的技术, 以促进训练更加鲁棒的机器学习模型.</p>
        
          <p class="article-more-link">
            <a href="2022/11/03/AAAI22/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E6%95%B0%E6%8D%AE%E4%B8%BA%E4%B8%AD%E5%BF%83/" rel="tag">数据为中心</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E6%AF%94%E8%B5%9B/" rel="tag">比赛</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-VQGAN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2021/12/16/VQGAN/" class="article-date">
  <time class="dt-published" datetime="2021-12-16T07:37:36.000Z" itemprop="datePublished">2021-12-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E8%AE%BA%E6%96%87/">论文</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2021/12/16/VQGAN/">Taming Transformers for High-Resolution Image Synthesis</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/155233-854490.png" alt="image-20211216155217239"></p>
<h2 id="abstract">Abstract</h2>
<p>设计学习远程交互顺序数据, 变压器继续显示最先进的结果, 各种任务. 与cnn不同的是, 它们不包含优先考虑本地互动的归纳偏见. 这使得它们具有表达能力, 但在计算上也不适合长序列, 比如高分辨率图像. 我们演示了如何将cnn的归纳偏差的有效性与变压器的表现力相结合, 使它们能够建模, 从而合成高分辨率图像. 我们展示了如何(i)使用cnn学习图像成分的context-rich词汇表, 反过来(ii)利用transformer在高分辨率图像中有效地建模它们的成分. 我们的方法很容易应用于条件合成任务, 其中非空间信息(如对象类)和空间信息(如分割)都可以控制生成的图像. 特别地, 我们给出了第一个用变换进行语义引导的百万像素图像合成的结果, 并获得了类条件ImageNet上自回归模型的最新进展.</p>
<h2 id="approach">Approach</h2>
<p>以前的工作[55,8]是将Transformer应用于图像生成, 对于大小为64×64pixels的图像, 结果很有希望, 但由于序列长度的成本是二次增加的, 不能简单地缩放到更高的分辨率. 高分辨率图像合成需要一个模型, 该模型能够理解图像的组成, 使其能够生成局部真实以及全局一致的模式. 因此, 我们不使用像素表示图像, 而是将其表示为一个码本中感知丰富的图像成分的组合. 通过学习一种有效的代码, 如第3.1节所述, 我们可以显著地减少合成的描述长度, 这使得我们可以有效地利用第3.2节所述的变压器架构在图像中建模它们的全局相互关系. 这种方法, 如图2所示, 能够在无条件和条件设置下生成真实和一致的高分辨率图像.</p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/160416-53601.png" alt="image-20211216160416427"></p>
<h3 id="learning-an effective codebook of image constituents for use in transformers">Learning an Effective Codebook of Image Constituents for Use in Transformers</h3>
<ul>
<li>
<p>image <span class="markdown-them-math-inline">$x\in \Bbb{R}^{h\times w\times 3}$</span> to <span class="markdown-them-math-inline">$z_q\in \Bbb{R}^{h\times w\times n_z}$</span>,<span class="markdown-them-math-inline">$n_z$</span> is the dim of codes .</p>
</li>
<li>
<p>codebook <span class="markdown-them-math-inline">$Z={z_k}_{k=1}^K$</span></p>
</li>
<li>
<p><span class="markdown-them-math-inline">$\hat{x}=G(z_q)$</span></p>
</li>
<li>
<p><span class="markdown-them-math-inline">$\hat{z}=E(x)$</span></p>
</li>
<li>
<p><span class="markdown-them-math-inline">$z_q=q(\hat{z}):=(\arg{min}_{z_k\in Z}||\hat{z}_{i,j}-z_k||)$</span></p>
</li>
<li>
<p>loss <span class="markdown-them-math-inline">$L_{VQ}(E,G,Z)=||x-\hat{x}||^2+||sg[E(x)]-z_q||^2_2+||sg[z_q]-E(x)||^2_2$</span>, <span class="markdown-them-math-inline">$sg[]$</span> denotes stop-gradient operation</p>
</li>
</ul>
<h4 id="learning-a perceptually rich codebook">Learning a Perceptually Rich Codebook</h4>
<p>使用Transformer来表示图像作为潜在图像成分的分布, 需要我们推动压缩的极限和学习丰富的码本. 为此, 我们提出了vqgan, 一种原始VQV AE的变体, 并使用一个鉴别器和感知损失[40,30,39,17,47], 以在增加压缩率的情况下保持良好的感知质量. 需要注意的是, 这与之前的工作不同, 之前的工作是基于像素[71,61]和基于变压器的自回归模型[8]在一个浅层量化模型之上. 更具体地说, 我们将[72]中lrecby使用的theL2loss替换为知觉损失, 并引入一个基于补丁的判别器[28]的对抗训练程序, 目的是区分真实图像和重建图像:</p>
<ul>
<li>
<p><span class="markdown-them-math-inline">$L_{GAN}(\{E, G,Z\}, D) = [logD(x) + log(1−D(\hat{x}))]$</span></p>
</li>
<li>
<p><span class="markdown-them-math-inline">$Q^*=\arg{min}_{E,G,Z}{max}_{D}\Bbb{E}_{x~p(x)}[L_{VQ}(E,G,Z)+\lambda L_{GAN}(\{E,G,Z\},D)]$</span></p>
</li>
<li>
<p><span class="markdown-them-math-inline">$\lambda=\frac{\nabla_{G_L}[L_rec]}{\nabla_{G_GAN}[L_rec]+\delta}$</span></p>
</li>
</ul>
<h3 id="learning-the composition of images with transformers">Learning the Composition of Images with Transformers</h3>
<h4 id="latent-transformers">Latent Transformers</h4>
<p>Given indices <span class="markdown-them-math-inline">$s&lt;i$</span>, the transformer learns to predict the distribution of possible next indices,</p>
<p><span class="markdown-them-math-inline">$p(s) =\prod_ip(s_i|s_{&lt;i})$</span></p>
<p><span class="markdown-them-math-inline">$L_{Transformer}=\Bbb{E}_{x∼p(x)}[−\log{p(s)}]$</span></p>
<h4 id="conditioned-synthesis">Conditioned Synthesis</h4>
<p><span class="markdown-them-math-inline">$p(s) =\prod_ip(s_i|s_{&lt;i},c)$</span></p>
<h4 id="generating-high-resolution images">Generating High-Resolution Images</h4>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/162156-748526.png" alt="image-20211216161603421"></p>
<h2 id="experiments">Experiments</h2>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/162155-235067.png" alt="image-20211216161702818"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/162152-181974.png" alt="image-20211216161722969"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/161755-886867.png" alt="image-20211216161755213"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/161934-917364.png" alt="image-20211216161828226"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/162134-585430.png" alt="image-20211216161843590"></p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202112/16/162326-87745.png" alt="image-20211216161859596"></p>
<p><img src="C:%5CUsers%5CZJW%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20211216161925988.png" alt="image-20211216161925988"></p>
<h2 id="vqgan+clip">VQGAN+CLIP</h2>
<p><img src="https://pic2.zhimg.com/v2-3840c8e97af3448fd5c529dfab9f0099_b.jpg" alt="img"></p>
<h3 id="x-+ clip">X + CLIP</h3>
<p>VQGAN+CLIP 只是将图像生成器与 CLIP 相结合的一个例子. 但是, 可以用任何类型的生成器替换 VQGAN, 并且根据生成器的不同, 它仍然可以很好地工作. X + CLIP 的许多变体已经出现, 例如 StyleCLIP(网址: <a href="https://link.zhihu.com/?target=https%3A//github.com/orpatashnik/StyleCLIP">https://github.com/orpatashnik/StyleCLIP</a>)  (StyleGAN + CLIP) 、 CLIPDraw(网址: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2106.14843">https://arxiv.org/abs/2106.14843</a>)  (使用矢量艺术生成器) 、BigGAN + CLIP 等等. 甚至 还有使用音频而不是图像的AudioCLIP (网址: <a href="https://link.zhihu.com/?target=https%3A//github.com/AndreyGuzhov/AudioCLIP">https://github.com/AndreyGuzhov/AudioCLIP</a>).</p>
<p><img src="https://pic1.zhimg.com/v2-b5784d58c9f081cb9b03b38373d64e28_b.jpg" alt="img"></p>
<p>图片: 使用 StyleCLIP 进行图像编辑</p>
<p><img src="https://pic2.zhimg.com/v2-84af8955dce4afb6440f5207fa06c1f5_b.jpg" alt="img"></p>
<p><video src="https://vdn1.vzuu.com/SD/2742466c-d706-11eb-8bab-f2503ac034e9.mp4?disable_local_cache=1&auth_key=1639646596-0-0-ea8095168f978bae6b1b309e3aeae852&f=mp4&bu=pico&expiration=1639646596&v=hw" controls="controls" width="500" height="300">您的浏览器不支持播放该视频! </video></p>
<p>加上 “3D photo inpainting” 竟然可以生成立体构图</p>
<p><video src="https://vdn1.vzuu.com/SD/275462ac-d706-11eb-85e2-da1003eb494f.mp4?disable_local_cache=1&auth_key=1639646587-0-0-543317f3d824b504110b69533397a7ef&f=mp4&bu=pico&expiration=1639646587&v=hw" controls="controls" width="500" height="300">您的浏览器不支持播放该视频! </video></p>
<p>甚至能用来猜猜那些从未露面的大佬们, 比如神秘的比特币之父中本聪</p>
<p><img src="https://pic2.zhimg.com/v2-4b7380f37007bd92247ae617bfc82759_b.jpg" alt="img"></p>
<p><video class="ztext-gif GifPlayer-gif2mp4" src="https://vdn3.vzuu.com/SD/273a65e6-d706-11eb-bcd2-da8b61ae19d4.mp4?disable_local_cache=1&amp;auth_key=1639646772-0-0-242d526345d82199ec642124f801faee&amp;f=mp4&amp;bu=pico&amp;expiration=1639646772&amp;v=tx" data-thumbnail="https://pic3.zhimg.com/v2-282a872f4ba46f4b46b760d3d89af9d2_b.jpg" poster="https://pic3.zhimg.com/v2-282a872f4ba46f4b46b760d3d89af9d2_b.jpg" data-size="normal" preload="metadata" loop="" playsinline=""></video></p>
<p><video class="_1k7bcr7" preload="metadata" playsinline="" webkit-playsinline="" x-webkit-airplay="deny" src="https://vdn1.vzuu.com/LD/bbd326b4-3aea-11ec-8592-66e4ae02c6b3-v4_t111-vlbchuzBoD.mp4?disable_local_cache=1&amp;auth_key=1639646964-0-0-0fef7b84e785ae5552df3de6253fec88&amp;f=mp4&amp;bu=http-com&amp;expiration=1639646964&amp;v=hw" style="object-fit: contain;"></video></p>
<p><img src="https://pic2.zhimg.com/v2-9a1cdb24655534997792c2aff7c6e7dd_b.jpg" alt="img"></p>
<h2 id="vqgan+transformer">VQGAN+transformer</h2>
<p>dall-e+VQGAN</p>
<p><img src="https://user-images.githubusercontent.com/3994972/125195424-248ac280-e21b-11eb-8231-cd9cede6d549.png" alt="coco_trained"></p>
<p><img src="https://user-images.githubusercontent.com/3994972/125196859-12138780-e221-11eb-8c63-a5ffef6615e8.png" alt="cocosamples2"></p>
<p><img src="https://user-images.githubusercontent.com/12422441/113150373-f76b6300-926e-11eb-86cc-3d920da2f46b.png" alt="image"></p>
<p><img src="https://user-images.githubusercontent.com/12422441/113150647-40bbb280-926f-11eb-8975-55968e2afb09.png" alt="image"></p>
<p><img src="C:%5CUsers%5CZJW%5CDownloads%5Cprogress.png" alt="progress"></p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/Transformer/" rel="tag">Transformer</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/codebook/" rel="tag">codebook</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/" rel="tag">生成算法</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-Multimodal Conditional Image Synthesis with Product-of-Experts GANs" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2021/12/14/Multimodal%20Conditional%20Image%20Synthesis%20with%20Product-of-Experts%20GANs/" class="article-date">
  <time class="dt-published" datetime="2021-12-14T07:57:15.000Z" itemprop="datePublished">2021-12-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E8%AE%BA%E6%96%87/">论文</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2021/12/14/Multimodal%20Conditional%20Image%20Synthesis%20with%20Product-of-Experts%20GANs/">Multimodal Conditional Image Synthesis with Product-of-Experts GANs</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://deepimagination.cc/PoE-GAN/">Multimodal Conditional Image Synthesis with Product-of-Experts GANs (deepimagination.cc)</a></p>
<h2 id="abstract">Abstract</h2>
<p>现有的条件图像合成框架基于单一模式的用户输入生成图像, 例如文本、分割、草图或样式参考. 当可用时, 它们通常无法利用多模态用户输入, 这降低了它们的实用性. 为了解决这一限制, 我们提出了 Product-of-Experts Generative Adversarial Networks (PoE-GAN) 框架, 该框架可以合成基于多个输入模式或其中任意子集(甚至空集)的图像. PoE-GAN由 product-of-experts  generator和多模态多尺度投影鉴别器组成. 通过我们精心设计的训练方案, PoE-GAN学会了合成高质量和多样性的图像. 除了在多模态条件图像合成方面的进步, PoE-GAN在单模态环境下的测试结果也优于现有的最佳单模态条件图像合成方法.</p>
<h2 id="product-of-experts-gans">Product-of-experts GANs</h2>
<ul>
<li>images <span class="markdown-them-math-inline">$x$</span> paired with <span class="markdown-them-math-inline">$M$</span> different input modalities <span class="markdown-them-math-inline">$(y_1,y_2,...,y_M)$</span></li>
</ul>
<h3 id="product-of-experts-modeling">Product-of-experts modeling</h3>
<p>…</p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/text-to-image/" rel="tag">text to image</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" rel="tag">生成模型</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-NÜWA Visual Synthesis Pre-training forNeural visUalWorld creAtion" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2021/11/28/N%C3%9CWA%20Visual%20Synthesis%20Pre-training%20forNeural%20visUalWorld%20creAtion/" class="article-date">
  <time class="dt-published" datetime="2021-11-28T06:22:55.000Z" itemprop="datePublished">2021-11-28</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E8%AE%BA%E6%96%87/">论文</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2021/11/28/N%C3%9CWA%20Visual%20Synthesis%20Pre-training%20forNeural%20visUalWorld%20creAtion/">NÜWA Visual Synthesis Pre-training for Neural visual world creation</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="3d-data representation">3D Data Representation</h2>
<p>unified 3D notation <span class="markdown-them-math-inline">$X\in \Bbb{R}^{h\times w\times s\times d}$</span>, <span class="markdown-them-math-inline">$h$</span> and <span class="markdown-them-math-inline">$w$</span> denote the number of tokens in spatial axis <span class="markdown-them-math-inline">$s$</span> denotes the number of tokens in the temporal axis , <span class="markdown-them-math-inline">$d$</span> is the dim of each token.</p>
<ul>
<li>
<p>text use lower-cased byte pair encoding (BPE) to token and embed them to <span class="markdown-them-math-inline">$\Bbb{R}^{1\times 1\times s\times d}$</span></p>
</li>
<li>
<p>images  using 1D VQ-GAN to encode to <span class="markdown-them-math-inline">$\Bbb{R}^{h\times w\times 1\times d}$</span></p>
</li>
<li>
<p>video using 2D VQ-GAN to encode to <span class="markdown-them-math-inline">$\Bbb{R}^{h\times w\times s\times d}$</span></p>
</li>
</ul>
<h2 id="3d-nearby self-attention">3D Nearby Self-Attention</h2>
<p>a coordinate <span class="markdown-them-math-inline">$(i,j,k)$</span> under <span class="markdown-them-math-inline">$X$</span> ,<span class="markdown-them-math-inline">$(\lfloor i \frac{h'}{h} \rfloor,\lfloor j \frac{w'}{w} \rfloor,\lfloor k \frac{s'}{s} \rfloor)$</span>,  a width, height and temporal extent <span class="markdown-them-math-inline">$e^w,e^h,e^s\in \Bbb{R}^+$</span>.</p>
<div class="markdown-them-math-block">$$N^{(i,j,k)}=\left\{ C_{abc}| \left|a-i' \right|\le e^h,\left|b-j' \right|\le e^w,\left|c-k' \right|\le e^s\right\}\in \Bbb{R}^{e^h\times e^w\times e^s \times d^{in}}
$$</div><div class="markdown-them-math-block">$$Q^{(i,j,k)}=XW^Q\\K^{(i,j,k)}=N^{(i,j,k)}W^K\\V^{(i,j,k)}=N^{(i,j,k)}W^V\\y_{ijk}=softmax(\frac{(Q^{(i,j,k)})^TK^{(i,j,k)}}{\sqrt{d^{in}}})V^{(i,j,k)}
$$</div><h2 id="3d-encoder-decoder">3D Encoder-Decoder</h2>
<div class="markdown-them-math-block">$$Y_{i j k}:=Y_{i j k}+P_{i}^{h}+P_{j}^{w}+P_{k}^{s}\\
C_{i j k}:=C_{i j k}+P_{i}^{h^{\prime}}+P_{j}^{w^{\prime}}+P_{k}^{s^{\prime}}
$$</div><div class="markdown-them-math-block">$$C^{(l)}=3DNA(C^{(l-1)},c^{(l-1)})
$$</div><div class="markdown-them-math-block">$$Y^{(l)}_{ijk}=3DNA(Y^{l-1}_{&lt;i,&lt;j,&lt;k},Y^{l-1}_{&lt;i,&lt;j,&lt;k})+3DNA(Y^{l-1}_{&lt;i,&lt;j,&lt;k},C^{L})
$$</div><p>inital <span class="markdown-them-math-inline">$V^{(1)}_{0,0,0}$</span> is a special <code>&lt; bos &gt;</code> token learned</p>
<p><img src="https://gitee.com/ZhaoJW11/typroimg/raw/master/image/202111/28/162124-463286.png" alt="image-20211128160612935"></p>
<h2 id="implementation-details">Implementation Details</h2>
<ul>
<li>text <span class="markdown-them-math-inline">$(e^w,e^h,e^s)=(1,1,\infty)$</span> <span class="markdown-them-math-inline">$1,1,77,1280$</span></li>
<li>image <span class="markdown-them-math-inline">$(e^w,e^h,e^s)=(3,3,1)$</span> <span class="markdown-them-math-inline">$21,21,1,1280$</span></li>
<li>video <span class="markdown-them-math-inline">$(e^w,e^h,e^s)=(3,3,3)$</span> <span class="markdown-them-math-inline">$21,21,10,1280$</span></li>
</ul>
<p>We pre-train on <strong>64 A100 GPUs</strong> for <strong>two weeks</strong> with the layer <span class="markdown-them-math-inline">$L$</span> in  set to 24, an Adam optimizer with a learning rate of 1e-3, a batch size of 128, and warm-up 5% of a total of 50M steps. The final pre-trained model has a total number of 870M parameters.</p>

      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/" rel="tag">生成模型</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-594. 最长和谐子序列" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="2021/11/20/594.%20%E6%9C%80%E9%95%BF%E5%92%8C%E8%B0%90%E5%AD%90%E5%BA%8F%E5%88%97/" class="article-date">
  <time class="dt-published" datetime="2021-11-20T04:54:37.000Z" itemprop="datePublished">2021-11-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="categories/%E7%AE%97%E6%B3%95/">算法</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="2021/11/20/594.%20%E6%9C%80%E9%95%BF%E5%92%8C%E8%B0%90%E5%AD%90%E5%BA%8F%E5%88%97/">594. 最长和谐子序列</a>
    </h1>
  

      </header>
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://leetcode-cn.com/problems/longest-harmonious-subsequence/">594. 最长和谐子序列 - 力扣 (LeetCode)  (leetcode-cn.com)</a></p>
<p>和谐数组是指一个数组里元素的最大值和最小值之间的差别 正好是 <strong>1</strong>.</p>
<p>现在, 给你一个整数数组<span class="markdown-them-math-inline">$ nums $</span>, 请你在所有可能的子序列中找到最长的和谐子序列的长度.</p>
<p>数组的子序列是一个由数组派生出来的序列, 它可以通过删除一些元素或不删除元素、且不改变其余元素的顺序而得到.</p>
        
          <p class="article-more-link">
            <a href="2021/11/20/594.%20%E6%9C%80%E9%95%BF%E5%92%8C%E8%B0%90%E5%AD%90%E5%BA%8F%E5%88%97/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" rel="tag">哈希表</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="tags/%E6%9E%9A%E4%B8%BE/" rel="tag">枚举</a></li></ul>

    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="page/2/">2</a><a class="extend next" rel="next" href="page/2/">下一页 &raquo;</a>
  </nav>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 Zhaojiangwei<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a><br>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="index.html" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="js/jquery-1.4.3.min.js"></script>


  
<script src="fancybox/jquery.fancybox-1.3.4.js"></script>




<script src="js/script.js"></script>






<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </div>
</body>
</html>